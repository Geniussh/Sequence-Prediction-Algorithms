{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2i(a):\n",
    "        return int(ord(a)-ord('a'))\n",
    "def i2l(i):\n",
    "    if i >= 0:\n",
    "        return chr(i+ord('a'))\n",
    "    else:\n",
    "        return '_'\n",
    "def iors(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError: # if it is a string, return a string\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the entire dataset into lists or list of lists\n",
    "def read_OCR(filename, n_features):\n",
    "    F = open(filename)\n",
    "    dataset = {}\n",
    "    dataset['ids'] = []#np.zeros(n_examples, dtype=int)\n",
    "    dataset['labels'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['labelDic'] = {} # To profile the distribution of labels\n",
    "    dataset['next_ids'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['word_ids'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['positions'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['folds'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['features'] = []#np.zeros([n_examples,n_features])\n",
    "    \n",
    "    for str_line in F.readlines():\n",
    "        #line0 = map(iors, filter(None, re.split('\\t', str_line.strip())))\n",
    "        ## ATTENTION: If you are using Python3, use the following line instead\n",
    "        line0 = list(map(iors, filter(None, re.split('\\t', str_line.strip()))))\n",
    "\n",
    "\n",
    "        dataset['ids'].append(int(line0.pop(0)))\n",
    "        dataset['labels'].append(l2i(line0.pop(0))) # The label is converted into integer('a'=>0, 'z'=>25)\n",
    "        if dataset['labels'][-1] in dataset['labelDic']:\n",
    "            dataset['labelDic'][dataset['labels'][-1]] += 1\n",
    "        else:\n",
    "            dataset['labelDic'][dataset['labels'][-1]] = 1\n",
    "            \n",
    "        dataset['next_ids'].append(int(line0.pop(0)))\n",
    "        dataset['word_ids'].append(int(line0.pop(0)))\n",
    "        dataset['positions'].append(int(line0.pop(0)))\n",
    "        dataset['folds'].append(int(line0.pop(0)))\n",
    "        if len(line0) != 128:  # Sanity check of the length\n",
    "            print (len(line0))\n",
    "        dataset['features'].append(line0)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoContext(object):\n",
    "    def __init__(self, unstructured_data, n_classes, n_iter, w_size, split):\n",
    "        self.Nu, self.du = unstructured_data.shape\n",
    "        self.unstructured_data = unstructured_data\n",
    "        self.n_classes = n_classes\n",
    "        self.num_iterations = n_iter\n",
    "        self.window_size = w_size\n",
    "        self.forest = None\n",
    "        self.models = []\n",
    "        self.split = split\n",
    "        \n",
    "        self.train, self.test, self.N, self.dtr, self.Ntr, self.Ntst = self.prep_ocr_data()\n",
    "        self.test_labels = np.zeros((self.Ntst))\n",
    "\n",
    "    def prep_ocr_data(self, fold=[0, 1], test_fold=9):\n",
    "        if type(fold) is int:\n",
    "            fold = [fold]\n",
    "        if type(test_fold) is int:\n",
    "            test_fold = [test_fold]\n",
    "        \n",
    "        if self.split == 1000:\n",
    "            fold = [0, 5]\n",
    "            test_fold = [2, 3, 4, 6, 7, 8]\n",
    "        elif self.split == 2500:\n",
    "            fold = [0, 1, 2, 3]\n",
    "            test_fold = [4, 5, 8, 9]\n",
    "        elif self.split == 4000:\n",
    "            fold = [0, 1, 2, 3, 4, 5]\n",
    "            test_fold = [6]\n",
    "\n",
    "        selected_unstructured_data = self.unstructured_data\n",
    "        Nt, du = selected_unstructured_data.shape\n",
    "        dt = du - 6\n",
    "        data_tmp = np.zeros((1, dt + 1))\n",
    "        train = []\n",
    "        test  = []\n",
    "        Ntr = 0\n",
    "        Ntst = 0\n",
    "\n",
    "        for i in range(Nt):\n",
    "            y = selected_unstructured_data[i, 1]    # scalar encoding of label\n",
    "            data_tmp = np.vstack((data_tmp, np.hstack((selected_unstructured_data[i, 6:], y))))\n",
    "            data_len = data_tmp.shape[0] - 1\n",
    "            if selected_unstructured_data[i, 2] == -1:\n",
    "                if selected_unstructured_data[i, 5] in fold:\n",
    "                    train.append(data_tmp[1:, :])\n",
    "                    Ntr += data_len\n",
    "                elif selected_unstructured_data[i, 5] in test_fold:\n",
    "                    test.append(data_tmp[1:, :])\n",
    "                    Ntst += data_len\n",
    "                data_tmp = np.zeros((1, dt + 1))\n",
    "\n",
    "        return train, test, Nt, dt, Ntr, Ntst\n",
    "    \n",
    "#     def prep_ocr_data2(self):\n",
    "#         split = self.split\n",
    "#         selected_unstructured_data = self.unstructured_data\n",
    "#         Nt, du = selected_unstructured_data.shape\n",
    "#         dt = du - 6\n",
    "#         data_tmp = np.zeros((1, dt + 1))\n",
    "#         train = []\n",
    "#         test  = []\n",
    "#         Ntr = 0\n",
    "#         Ntst = 0\n",
    "#         count = 0\n",
    "#         for i in range(Nt):\n",
    "#             y = selected_unstructured_data[i, 1]\n",
    "#             data_tmp = np.vstack((data_tmp, np.hstack((selected_unstructured_data[i, 6:], y)) ) )\n",
    "#             data_len = data_tmp.shape[0] - 1\n",
    "#             if selected_unstructured_data[i, 2] == -1:\n",
    "#                 if count < split:\n",
    "#                     train.append(data_tmp[1:, :])\n",
    "#                     Ntr += data_len\n",
    "#                 elif count >= split and count < 5000:\n",
    "#                     test.append(data_tmp[1:, :])\n",
    "#                     Ntst += data_len\n",
    "#                 data_tmp = np.zeros((1, dt + 1))\n",
    "#                 count += 1\n",
    "#                 if count == 5000: break\n",
    "#         return train, test, Nt, dt, Ntr, Ntst\n",
    "\n",
    "    def train(self):\n",
    "        if self.train is None:\n",
    "            self.prep_ocr_data()\n",
    "\n",
    "        confidence = np.zeros((self.Ntr, self.n_classes))\n",
    "        accurracy1 = []\n",
    "        accurracy2 = []\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            print('Iteration number ' + str(i+1) + ' out of ' + str(self.num_iterations))\n",
    "            W = np.zeros((self.Ntr, self.dtr + self.n_classes * self.window_size * 2))  # Weight matrix: X + confidence\n",
    "            Y = np.zeros(self.Ntr)                                                      # Cached predictions\n",
    "\n",
    "            curr_line = 0\n",
    "            for j in range(len(self.train)):\n",
    "                word = self.train[j]        # get current word (X, which consists of x_1, x_2, ... x_m)\n",
    "                word_len = word.shape[0]    # find num letters in X (i.e. m)\n",
    "\n",
    "                W[curr_line:curr_line+word_len, :self.dtr] = word[:, :self.dtr]\n",
    "                W[curr_line:curr_line+word_len, self.dtr:] = self.extend_context(\n",
    "                        confidence[curr_line:curr_line+word_len, :]\n",
    "                )\n",
    "\n",
    "                Y[curr_line:curr_line+word_len] = self.train[j][:, -1]\n",
    "                curr_line += word_len\n",
    "\n",
    "            # Build model\n",
    "            svm_class = svm.LinearSVC(multi_class='crammer_singer', random_state=42) \n",
    "            svm_class.fit(W, Y)\n",
    "\n",
    "            self.models.append((svm_class, W))\n",
    "\n",
    "            # Prediction\n",
    "            if i < self.num_iterations:\n",
    "                acc1, acc2, confidence = self.svm_inference(self.train, confidence, svm_class)\n",
    "                accurracy1.append(acc1)\n",
    "                accurracy2.append(acc2)\n",
    "\n",
    "        return accurracy1, accurracy2, confidence\n",
    "\n",
    "    def svm_inference(self, data, confidence, svm, norm=True, in_test=False):\n",
    "        Nt = len(data)\n",
    "#         print(Nt)\n",
    "        acc1 = 0\n",
    "        acc2 = 0\n",
    "        total1 = 0\n",
    "        total2 = 0\n",
    "        conf_new = np.zeros(confidence.shape)\n",
    "\n",
    "        cur_line = 0\n",
    "        for i in range(Nt):\n",
    "\n",
    "            word = data[i]\n",
    "            word_len = word.shape[0]\n",
    "\n",
    "            Y = word[:, -1]\n",
    "\n",
    "            if in_test:\n",
    "                self.test_labels[cur_line:cur_line+word_len] = Y\n",
    "\n",
    "            W_prime = np.zeros((word_len, self.dtr + self.n_classes * self.window_size * 2))\n",
    "            W_prime[:, :self.dtr] = word[:, :self.dtr]\n",
    "            W_prime[:, self.dtr:] = self.extend_context(confidence[cur_line:(cur_line + word_len), :])\n",
    "\n",
    "            conf = svm.decision_function(W_prime)   # Confidence measures of predictions\n",
    "\n",
    "            if norm:\n",
    "                conf = (1 + np.exp(-1*conf))**-1    # Sigmoid function --> Normalization\n",
    "\n",
    "            conf_new[cur_line : cur_line+word_len, :] = conf\n",
    "            cur_line += word_len\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total1 += word_len\n",
    "            total2 += 1\n",
    "            subtask_acc = svm.score(W_prime, Y)\n",
    "            acc2 += subtask_acc\n",
    "            acc1 += subtask_acc * word_len\n",
    "\n",
    "        return acc1/total1, acc2/total2, conf_new\n",
    "\n",
    "    def svm_predict(self, test_data=None):\n",
    "        if test_data is None:\n",
    "            test_data = self.test\n",
    "        n_iter = len(self.models)\n",
    "\n",
    "        confidence = np.zeros((self.Ntst, self.n_classes))\n",
    "\n",
    "        accuracy1 = []\n",
    "        accuracy2 = []\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            curr_model, _ = self.models[i]\n",
    "            acc1, acc2, confidence = self.svm_inference(test_data, confidence, curr_model, True, True)\n",
    "            accuracy1.append(acc1)\n",
    "            accuracy2.append(acc2)\n",
    "\n",
    "        return accuracy1, accuracy2, confidence\n",
    "\n",
    "    def extend_context(self, conf, window_size=None, n_classes=None):\n",
    "        if window_size is None:\n",
    "            window_size = self.window_size\n",
    "        if n_classes is None:\n",
    "            n_classes = self.n_classes\n",
    "\n",
    "        word_len = conf.shape[0]\n",
    "        W = np.zeros((word_len, 2*window_size*n_classes))\n",
    "        for i in range(word_len):\n",
    "            for w in range(-window_size, window_size):\n",
    "                if 0 <= i + w < word_len:\n",
    "                    if w < 0:\n",
    "                        W[i, (window_size + w)*n_classes : (window_size+w)*n_classes + n_classes] =\\\n",
    "                            conf[i + w, :n_classes]\n",
    "                    elif w > 0:\n",
    "                        W[i, (window_size + w - 1)*n_classes : (window_size + w - 1)*n_classes + n_classes] =\\\n",
    "                            conf[i + w, :n_classes]\n",
    "\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = read_OCR('../letter.data', 128)\n",
    "letter_data = np.hstack(((np.array(dataset1['ids']).reshape(-1,1)), (np.array(dataset1['labels']).reshape(-1,1)), \n",
    "        (np.array(dataset1['next_ids']).reshape(-1,1)), (np.array(dataset1['word_ids']).reshape(-1,1)), \n",
    "        (np.array(dataset1['positions']).reshape(-1,1)), (np.array(dataset1['folds']).reshape(-1,1)), \n",
    "        np.array(dataset1['features'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1000/4000 Split\n",
    "\n",
    "j = 4\n",
    "test_accuracies1 = np.zeros((1, j))\n",
    "test_accuracies2 = np.zeros((1, j))\n",
    "# Hyper-parameters: \n",
    "# i: window size\n",
    "# j: number of iterations\n",
    "for i in range(1, 4):\n",
    "\n",
    "    print('Creating AutoContext object, prepping OCR dataset')\n",
    "    ac = AutoContext(letter_data,26,j,i,1000)\n",
    "    # print(ac.train[1].shape)  # sanity check\n",
    "    # print(ac.Ntr, ac.dtr)\n",
    "\n",
    "    print('Training Strategy 2: SVM-based Auto Context')\n",
    "    tr_accuracy1, tr_accuracy2, conf = ac.train()\n",
    "    print('Training accuracy (by word and letter):')\n",
    "    print(tr_accuracy1)\n",
    "    print(tr_accuracy2)\n",
    "    print('Testing Strategy 2')\n",
    "    ts_accuracy1, ts_accuracy2, conf = ac.svm_predict()\n",
    "    print('Testing accuracy (by word and by letter):')\n",
    "    print(ts_accuracy1)\n",
    "    print(ts_accuracy2)\n",
    "    test_accuracies1 = np.vstack((test_accuracies1, ts_accuracy1))\n",
    "    test_accuracies2 = np.vstack((test_accuracies2, ts_accuracy2))\n",
    "\n",
    "print(test_accuracies1)\n",
    "print(test_accuracies2)\n",
    "\n",
    "np.savetxt('accuracies1', test_accuracies1[1:,:], '%.5f')\n",
    "np.savetxt('accuracies2', test_accuracies2[1:,:], '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2500/2500 Split\n",
    "\n",
    "j = 4\n",
    "train_accuracies = np.zeros((1, j))\n",
    "test_accuracies1 = np.zeros((1, j))\n",
    "test_accuracies2 = np.zeros((1, j))\n",
    "# Hyper-parameters: \n",
    "# i: window size\n",
    "# j: number of iterations\n",
    "for i in range(1, 4):\n",
    "\n",
    "    print('Creating AutoContext object, prepping OCR dataset')\n",
    "    ac = AutoContext(letter_data,26,j,i,2500)\n",
    "    # print(ac.train[1].shape)  # sanity check\n",
    "    # print(ac.Ntr, ac.dtr)\n",
    "\n",
    "    print('Training Strategy 2: SVM-based Auto Context')\n",
    "    tr_accuracy1, tr_accuracy2, conf = ac.train()\n",
    "    print('Training accuracy (by word and letter):')\n",
    "    print(tr_accuracy1)\n",
    "    print(tr_accuracy2)\n",
    "    train_accuracies = np.vstack((train_accuracies, tr_accuracy2))  # token accuracy\n",
    "    print('Testing Strategy 2')\n",
    "    ts_accuracy1, ts_accuracy2, conf = ac.svm_predict()\n",
    "    print('Testing accuracy (by word and by letter):')\n",
    "    print(ts_accuracy1)\n",
    "    print(ts_accuracy2)\n",
    "    test_accuracies1 = np.vstack((test_accuracies1, ts_accuracy1))\n",
    "    test_accuracies2 = np.vstack((test_accuracies2, ts_accuracy2))\n",
    "\n",
    "print(test_accuracies1)\n",
    "print(test_accuracies2)\n",
    "#\n",
    "np.savetxt('accuracies1_55', test_accuracies1[1:,:], '%.5f')\n",
    "np.savetxt('accuracies2_55', test_accuracies2[1:,:], '%.5f')\n",
    "np.savetxt('tr_accuracies_55', train_accuracies[1:,:], '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4000/1000 Split\n",
    "\n",
    "j = 4\n",
    "train_accuracies = np.zeros((1, j))\n",
    "test_accuracies1 = np.zeros((1, j))\n",
    "test_accuracies2 = np.zeros((1, j))\n",
    "# Hyper-parameters: \n",
    "# i: window size\n",
    "# j: number of iterations\n",
    "for i in range(1, 4):\n",
    "\n",
    "    print('Creating AutoContext object, prepping OCR dataset')\n",
    "    ac = AutoContext(letter_data,26,j,i,4000)\n",
    "    # print(ac.train[1].shape)  # sanity check\n",
    "    # print(ac.Ntr, ac.dtr)\n",
    "\n",
    "    print('Training Strategy 2: SVM-based Auto Context')\n",
    "    tr_accuracy1, tr_accuracy2, conf = ac.train()\n",
    "    print('Training accuracy (by word and letter):')\n",
    "    print(tr_accuracy1)\n",
    "    print(tr_accuracy2)\n",
    "    train_accuracies = np.vstack((train_accuracies, tr_accuracy2))  # token accuracy\n",
    "    print('Testing Strategy 2')\n",
    "    ts_accuracy1, ts_accuracy2, conf = ac.svm_predict()\n",
    "    print('Testing accuracy (by word and by letter):')\n",
    "    print(ts_accuracy1)\n",
    "    print(ts_accuracy2)\n",
    "    test_accuracies1 = np.vstack((test_accuracies1, ts_accuracy1))\n",
    "    test_accuracies2 = np.vstack((test_accuracies2, ts_accuracy2))\n",
    "\n",
    "print(test_accuracies1)\n",
    "print(test_accuracies2)\n",
    "#\n",
    "np.savetxt('accuracies1_82', test_accuracies1[1:,:], '%.5f')\n",
    "np.savetxt('accuracies2_82', test_accuracies2[1:,:], '%.5f')\n",
    "np.savetxt('tr_accuracies_82', train_accuracies[1:,:], '%.5f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
