{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_file_name, n_features=None, n_datapoints=-1):\n",
    "    \"\"\"\n",
    "    Slightly Modified by Alex Rosengarten\n",
    "    Source: https://github.com/cjlin1/libsvm/blob/master/python/svmutil.py\n",
    "    svm_read_problem(data_file_name) -> [y, x]\n",
    "    Read LIBSVM-format data from data_file_name and return labels y\n",
    "    and data instances x.\n",
    "    \"\"\"\n",
    "    prob_y = []\n",
    "    prob_x = []\n",
    "    i = 0\n",
    "    for line in open(data_file_name):\n",
    "        if i is n_datapoints:\n",
    "            break\n",
    "        line = line.split(None, 1)\n",
    "        # In case an instance with all zero features\n",
    "        if len(line) == 1: line += ['']\n",
    "        label, features = line\n",
    "        if n_features is None:\n",
    "            xi = [0 for _ in range(len(features.split()))]\n",
    "        else:\n",
    "            xi = [0 for _ in range(n_features)]\n",
    "        for e in features.split():\n",
    "            ind, val = e.split(\":\")\n",
    "            xi[int(ind)-1] = float(val)\n",
    "        i += 1\n",
    "        prob_y += [float(label)]\n",
    "        prob_x += [xi]\n",
    "\n",
    "    return prob_y, prob_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    '''\n",
    "    A parallelized way of importing LIBSVM-formatted data from a file. To use, create a Data object with the proper\n",
    "    parameters and call read_data().\n",
    "    '''\n",
    "    def __init__(self, data_file_name, n_features=None, n_datapoints=-1, n_workers=None, filetype='SVM', delim=None):\n",
    "        '''\n",
    "        :param data_file_name: data file\n",
    "        :param n_features: number of features in the data set\n",
    "        :param n_datapoints: [non-functional feature] number of data points to import before stopping\n",
    "        :param n_workers: number of threads or processes working to import the data\n",
    "        :param filetype:\n",
    "        :param delim:\n",
    "        :return:\n",
    "        '''\n",
    "        self.file = data_file_name\n",
    "        self.n_features = n_features\n",
    "        self.n_datapoints = n_datapoints\n",
    "        self.n_threads = n_workers\n",
    "        self.filetype = filetype\n",
    "        self.delimiter = delim\n",
    "\n",
    "    def process_svm_line(self, line):\n",
    "        '''\n",
    "        Process one line of data from a SVM formated data file.\n",
    "        See following for example datasets:\n",
    "        http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html\n",
    "        :param line: the line of the file to process\n",
    "        :return: An array of data with the label at the right most column --> [X | y]\n",
    "        '''\n",
    "        line = line.split(None, 1)\n",
    "        # In case an instance with all zero features\n",
    "        if len(line) == 1: line += ['']\n",
    "        label, features = line\n",
    "        if self.n_features is None:\n",
    "            xi = [0.0 for _ in range(len(features.split()))]\n",
    "        else:\n",
    "            xi = [0.0 for _ in range(self.n_features)]\n",
    "        for e in features.split():\n",
    "            ind, val = e.split(\":\")\n",
    "            xi[int(ind)-1] = float(val)\n",
    "\n",
    "        return xi + [float(label)]\n",
    "\n",
    "    def process_csv_line(self, line):\n",
    "        return [self.determine_data_type(v) for v in line.strip().split(self.delimiter) if v is not None]\n",
    "\n",
    "    def determine_data_type(self, elem):\n",
    "        if self.is_float(elem) and '.' in elem:\n",
    "            return float(elem)\n",
    "        elif self.is_int(elem):\n",
    "            return int(elem)\n",
    "        else:\n",
    "            return str(elem)\n",
    "\n",
    "    def is_float(self, s):\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def is_int(self, s):\n",
    "        try:\n",
    "            int(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def is_complex(self, s):\n",
    "        try:\n",
    "            complex(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def read_data(self):\n",
    "        '''\n",
    "        Reads the data into program in parallel via a thread pool. Uses the higher-order function Map to call\n",
    "        process_line on the input file.\n",
    "        :return: dataset, in numpy array format.\n",
    "        '''\n",
    "        if self.n_threads is None:\n",
    "            pool = Pool()\n",
    "        else:\n",
    "            pool = Pool(self.n_threads)\n",
    "\n",
    "        with open(self.file) as f:\n",
    "            if self.filetype == 'SVM':\n",
    "                results = pool.map(self.process_svm_line, f)\n",
    "            else:\n",
    "                results = pool.map(self.process_csv_line, f)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoContext(object):\n",
    "    def __init__(self, unstructured_data, n_classes, n_iter, w_size, split):\n",
    "        self.Nu, self.du = unstructured_data.shape\n",
    "        self.unstructured_data = unstructured_data\n",
    "        self.n_classes = n_classes\n",
    "        self.num_iterations = n_iter\n",
    "        self.window_size = w_size\n",
    "        self.forest = None\n",
    "        self.models = []\n",
    "        self.split = split\n",
    "        \n",
    "        self.train, self.test, self.N, self.dtr, self.Ntr, self.Ntst = self.prep_ocr_data()\n",
    "        self.test_labels = np.zeros((self.Ntst))\n",
    "\n",
    "    def prep_ocr_data(self, fold=[0, 1], test_fold=9):\n",
    "        if type(fold) is int:\n",
    "            fold = [fold]\n",
    "        if type(test_fold) is int:\n",
    "            test_fold = [test_fold]\n",
    "        \n",
    "        if self.split == 1000:\n",
    "            fold = [0, 5]\n",
    "            test_fold = [2, 3, 4, 6, 7, 8]\n",
    "        elif self.split == 2500:\n",
    "            fold = [0, 1, 2, 3]\n",
    "            test_fold = [4, 5, 8, 9]\n",
    "        elif self.split == 4000:\n",
    "            fold = [0, 1, 2, 3, 4, 5]\n",
    "            test_fold = [6]\n",
    "\n",
    "        selected_unstructured_data = self.unstructured_data\n",
    "        Nt, du = selected_unstructured_data.shape\n",
    "        dt = du - 6\n",
    "        data_tmp = np.zeros((1, dt + 1))\n",
    "        train = []\n",
    "        test  = []\n",
    "        Ntr = 0\n",
    "        Ntst = 0\n",
    "\n",
    "        for i in range(Nt):\n",
    "            y = selected_unstructured_data[i, 1]    # scalar encoding of label\n",
    "            data_tmp = np.vstack((data_tmp, np.hstack((selected_unstructured_data[i, 6:], y))))\n",
    "            data_len = data_tmp.shape[0] - 1\n",
    "            if selected_unstructured_data[i, 2] == -1:\n",
    "                if selected_unstructured_data[i, 5] in fold:\n",
    "                    train.append(data_tmp[1:, :])\n",
    "                    Ntr += data_len\n",
    "                elif selected_unstructured_data[i, 5] in test_fold:\n",
    "                    test.append(data_tmp[1:, :])\n",
    "                    Ntst += data_len\n",
    "                data_tmp = np.zeros((1, dt + 1))\n",
    "\n",
    "        return train, test, Nt, dt, Ntr, Ntst\n",
    "    \n",
    "#     def prep_ocr_data2(self):\n",
    "#         split = self.split\n",
    "#         selected_unstructured_data = self.unstructured_data\n",
    "#         Nt, du = selected_unstructured_data.shape\n",
    "#         dt = du - 6\n",
    "#         data_tmp = np.zeros((1, dt + 1))\n",
    "#         train = []\n",
    "#         test  = []\n",
    "#         Ntr = 0\n",
    "#         Ntst = 0\n",
    "#         count = 0\n",
    "#         for i in range(Nt):\n",
    "#             y = selected_unstructured_data[i, 1]\n",
    "#             data_tmp = np.vstack((data_tmp, np.hstack((selected_unstructured_data[i, 6:], y)) ) )\n",
    "#             data_len = data_tmp.shape[0] - 1\n",
    "#             if selected_unstructured_data[i, 2] == -1:\n",
    "#                 if count < split:\n",
    "#                     train.append(data_tmp[1:, :])\n",
    "#                     Ntr += data_len\n",
    "#                 elif count >= split and count < 5000:\n",
    "#                     test.append(data_tmp[1:, :])\n",
    "#                     Ntst += data_len\n",
    "#                 data_tmp = np.zeros((1, dt + 1))\n",
    "#                 count += 1\n",
    "#                 if count == 5000: break\n",
    "#         return train, test, Nt, dt, Ntr, Ntst\n",
    "\n",
    "    def train(self):\n",
    "        if self.train is None:\n",
    "            self.prep_ocr_data()\n",
    "\n",
    "        confidence = np.zeros((self.Ntr, self.n_classes))\n",
    "        accurracy1 = []\n",
    "        accurracy2 = []\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            print('Iteration number ' + str(i+1) + ' out of ' + str(self.num_iterations))\n",
    "            W = np.zeros((self.Ntr, self.dtr + self.n_classes * self.window_size * 2))  # Weight matrix: X + confidence\n",
    "            Y = np.zeros(self.Ntr)                                                      # Cached predictions\n",
    "\n",
    "            curr_line = 0\n",
    "            for j in range(len(self.train)):\n",
    "                word = self.train[j]        # get current word (X, which consists of x_1, x_2, ... x_m)\n",
    "                word_len = word.shape[0]    # find num letters in X (i.e. m)\n",
    "\n",
    "                W[curr_line:curr_line+word_len, :self.dtr] = word[:, :self.dtr]\n",
    "                W[curr_line:curr_line+word_len, self.dtr:] = self.extend_context(\n",
    "                        confidence[curr_line:curr_line+word_len, :]\n",
    "                )\n",
    "\n",
    "                Y[curr_line:curr_line+word_len] = self.train[j][:, -1]\n",
    "                curr_line += word_len\n",
    "\n",
    "            # Build model\n",
    "            svm_class = svm.LinearSVC(multi_class='crammer_singer', random_state=42) \n",
    "            svm_class.fit(W, Y)\n",
    "\n",
    "            self.models.append((svm_class, W))\n",
    "\n",
    "            # Prediction\n",
    "            if i < self.num_iterations:\n",
    "                acc1, acc2, confidence = self.svm_inference(self.train, confidence, svm_class)\n",
    "                accurracy1.append(acc1)\n",
    "                accurracy2.append(acc2)\n",
    "\n",
    "        return accurracy1, accurracy2, confidence\n",
    "\n",
    "    def svm_inference(self, data, confidence, svm, norm=True, in_test=False):\n",
    "        Nt = len(data)\n",
    "#         print(Nt)\n",
    "        acc1 = 0\n",
    "        acc2 = 0\n",
    "        total1 = 0\n",
    "        total2 = 0\n",
    "        conf_new = np.zeros(confidence.shape)\n",
    "\n",
    "        cur_line = 0\n",
    "        for i in range(Nt):\n",
    "\n",
    "            word = data[i]\n",
    "            word_len = word.shape[0]\n",
    "\n",
    "            Y = word[:, -1]\n",
    "\n",
    "            if in_test:\n",
    "                self.test_labels[cur_line:cur_line+word_len] = Y\n",
    "\n",
    "            W_prime = np.zeros((word_len, self.dtr + self.n_classes * self.window_size * 2))\n",
    "            W_prime[:, :self.dtr] = word[:, :self.dtr]\n",
    "            W_prime[:, self.dtr:] = self.extend_context(confidence[cur_line:(cur_line + word_len), :])\n",
    "\n",
    "            conf = svm.decision_function(W_prime)   # Confidence measures of predictions\n",
    "\n",
    "            if norm:\n",
    "                conf = (1 + np.exp(-1*conf))**-1    # Sigmoid function --> Normalization\n",
    "\n",
    "            conf_new[cur_line : cur_line+word_len, :] = conf\n",
    "            cur_line += word_len\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total1 += word_len\n",
    "            total2 += 1\n",
    "            subtask_acc = svm.score(W_prime, Y)\n",
    "            acc2 += subtask_acc\n",
    "            acc1 += subtask_acc * word_len\n",
    "\n",
    "        return acc1/total1, acc2/total2, conf_new\n",
    "\n",
    "    def svm_predict(self, test_data=None):\n",
    "        if test_data is None:\n",
    "            test_data = self.test\n",
    "        n_iter = len(self.models)\n",
    "\n",
    "        confidence = np.zeros((self.Ntst, self.n_classes))\n",
    "\n",
    "        accuracy1 = []\n",
    "        accuracy2 = []\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            curr_model, _ = self.models[i]\n",
    "            acc1, acc2, confidence = self.svm_inference(test_data, confidence, curr_model, True, True)\n",
    "            accuracy1.append(acc1)\n",
    "            accuracy2.append(acc2)\n",
    "\n",
    "        return accuracy1, accuracy2, confidence\n",
    "\n",
    "    def extend_context(self, conf, window_size=None, n_classes=None):\n",
    "        if window_size is None:\n",
    "            window_size = self.window_size\n",
    "        if n_classes is None:\n",
    "            n_classes = self.n_classes\n",
    "\n",
    "        word_len = conf.shape[0]\n",
    "        W = np.zeros((word_len, 2*window_size*n_classes))\n",
    "        for i in range(word_len):\n",
    "            for w in range(-window_size, window_size):\n",
    "                if 0 <= i + w < word_len:\n",
    "                    if w < 0:\n",
    "                        W[i, (window_size + w)*n_classes : (window_size+w)*n_classes + n_classes] =\\\n",
    "                            conf[i + w, :n_classes]\n",
    "                    elif w > 0:\n",
    "                        W[i, (window_size + w - 1)*n_classes : (window_size + w - 1)*n_classes + n_classes] =\\\n",
    "                            conf[i + w, :n_classes]\n",
    "\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_data_obj = Data('../../letter.data', filetype='CSV')\n",
    "letter_data = letter_data_obj.read_data()\n",
    "print('data loaded')\n",
    "\n",
    "# Convert characters to 1 byte integers\n",
    "letter_data[:,1] = [ord(c) - 97 for c in letter_data[:,1]]  # char --> intx\n",
    "letter_data = letter_data.astype('i1')\n",
    "print('data converted to ints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1000/4000 Split\n",
    "\n",
    "j = 4\n",
    "test_accuracies1 = np.zeros((1, j))\n",
    "test_accuracies2 = np.zeros((1, j))\n",
    "# Hyper-parameters: \n",
    "# i: window size\n",
    "# j: number of iterations\n",
    "for i in range(1, 4):\n",
    "\n",
    "    print('Creating AutoContext object, prepping OCR dataset')\n",
    "    ac = AutoContext(letter_data,26,j,i,1000)\n",
    "    # print(ac.train[1].shape)  # sanity check\n",
    "    # print(ac.Ntr, ac.dtr)\n",
    "\n",
    "    print('Training Strategy 2: SVM-based Auto Context')\n",
    "    tr_accuracy1, tr_accuracy2, conf = ac.train()\n",
    "    print('Training accuracy (by word and letter):')\n",
    "    print(tr_accuracy1)\n",
    "    print(tr_accuracy2)\n",
    "    print('Testing Strategy 2')\n",
    "    ts_accuracy1, ts_accuracy2, conf = ac.svm_predict()\n",
    "    print('Testing accuracy (by word and by letter):')\n",
    "    print(ts_accuracy1)\n",
    "    print(ts_accuracy2)\n",
    "    test_accuracies1 = np.vstack((test_accuracies1, ts_accuracy1))\n",
    "    test_accuracies2 = np.vstack((test_accuracies2, ts_accuracy2))\n",
    "\n",
    "print(test_accuracies1)\n",
    "print(test_accuracies2)\n",
    "\n",
    "np.savetxt('accuracies1', test_accuracies1[1:,:], '%.5f')\n",
    "np.savetxt('accuracies2', test_accuracies2[1:,:], '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2500/2500 Split\n",
    "\n",
    "j = 4\n",
    "train_accuracies = np.zeros((1, j))\n",
    "test_accuracies1 = np.zeros((1, j))\n",
    "test_accuracies2 = np.zeros((1, j))\n",
    "# Hyper-parameters: \n",
    "# i: window size\n",
    "# j: number of iterations\n",
    "for i in range(1, 4):\n",
    "\n",
    "    print('Creating AutoContext object, prepping OCR dataset')\n",
    "    ac = AutoContext(letter_data,26,j,i,2500)\n",
    "    # print(ac.train[1].shape)  # sanity check\n",
    "    # print(ac.Ntr, ac.dtr)\n",
    "\n",
    "    print('Training Strategy 2: SVM-based Auto Context')\n",
    "    tr_accuracy1, tr_accuracy2, conf = ac.train()\n",
    "    print('Training accuracy (by word and letter):')\n",
    "    print(tr_accuracy1)\n",
    "    print(tr_accuracy2)\n",
    "    train_accuracies = np.vstack((train_accuracies, tr_accuracy2))  # token accuracy\n",
    "    print('Testing Strategy 2')\n",
    "    ts_accuracy1, ts_accuracy2, conf = ac.svm_predict()\n",
    "    print('Testing accuracy (by word and by letter):')\n",
    "    print(ts_accuracy1)\n",
    "    print(ts_accuracy2)\n",
    "    test_accuracies1 = np.vstack((test_accuracies1, ts_accuracy1))\n",
    "    test_accuracies2 = np.vstack((test_accuracies2, ts_accuracy2))\n",
    "\n",
    "print(test_accuracies1)\n",
    "print(test_accuracies2)\n",
    "#\n",
    "np.savetxt('accuracies1_55', test_accuracies1[1:,:], '%.5f')\n",
    "np.savetxt('accuracies2_55', test_accuracies2[1:,:], '%.5f')\n",
    "np.savetxt('tr_accuracies_55', train_accuracies[1:,:], '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4000/1000 Split\n",
    "\n",
    "j = 4\n",
    "train_accuracies = np.zeros((1, j))\n",
    "test_accuracies1 = np.zeros((1, j))\n",
    "test_accuracies2 = np.zeros((1, j))\n",
    "# Hyper-parameters: \n",
    "# i: window size\n",
    "# j: number of iterations\n",
    "for i in range(1, 4):\n",
    "\n",
    "    print('Creating AutoContext object, prepping OCR dataset')\n",
    "    ac = AutoContext(letter_data,26,j,i,4000)\n",
    "    # print(ac.train[1].shape)  # sanity check\n",
    "    # print(ac.Ntr, ac.dtr)\n",
    "\n",
    "    print('Training Strategy 2: SVM-based Auto Context')\n",
    "    tr_accuracy1, tr_accuracy2, conf = ac.train()\n",
    "    print('Training accuracy (by word and letter):')\n",
    "    print(tr_accuracy1)\n",
    "    print(tr_accuracy2)\n",
    "    train_accuracies = np.vstack((train_accuracies, tr_accuracy2))  # token accuracy\n",
    "    print('Testing Strategy 2')\n",
    "    ts_accuracy1, ts_accuracy2, conf = ac.svm_predict()\n",
    "    print('Testing accuracy (by word and by letter):')\n",
    "    print(ts_accuracy1)\n",
    "    print(ts_accuracy2)\n",
    "    test_accuracies1 = np.vstack((test_accuracies1, ts_accuracy1))\n",
    "    test_accuracies2 = np.vstack((test_accuracies2, ts_accuracy2))\n",
    "\n",
    "print(test_accuracies1)\n",
    "print(test_accuracies2)\n",
    "#\n",
    "np.savetxt('accuracies1_82', test_accuracies1[1:,:], '%.5f')\n",
    "np.savetxt('accuracies2_82', test_accuracies2[1:,:], '%.5f')\n",
    "np.savetxt('tr_accuracies_82', train_accuracies[1:,:], '%.5f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
